{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3_sGGju6zTt"
      },
      "source": [
        "# Face Recognition Using Eigenfaces\n",
        "\n",
        "The following algorithm takes people's images with different emotions, and then uses the concept of Eigneface to find out similarity among them and further identify them. \n",
        "\n",
        "The image dataset is provided with repo, which is taken from [Yale Face Dataset](https://www.face-rec.org/databases/). Upload the whole image dataset here, and the following code generates a training and (exclusive) test set out of it. \n",
        "\n",
        "Note: The dataset used here is Yale Face Database (contains 165 grayscale images in GIF format of 15 individuals)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVxAAqZj6H3m",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "beec2b08-ccf3-4032-cd96-17c885d95850"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0a814db6-01bd-42dd-b7fd-0e5f0502d98e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0a814db6-01bd-42dd-b7fd-0e5f0502d98e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving subject01.centerlight to subject01.centerlight\n",
            "Saving subject01.glasses to subject01.glasses\n",
            "Saving subject01.happy to subject01.happy\n",
            "Saving subject01.leftlight to subject01.leftlight\n",
            "Saving subject01.noglasses to subject01.noglasses\n",
            "Saving subject01.normal to subject01.normal\n",
            "Saving subject01.rightlight to subject01.rightlight\n",
            "Saving subject01.sad to subject01.sad\n",
            "Saving subject01.sleepy to subject01.sleepy\n",
            "Saving subject01.surprised to subject01.surprised\n",
            "Saving subject01.wink to subject01.wink\n",
            "Saving subject02.centerlight to subject02.centerlight\n",
            "Saving subject02.glasses to subject02.glasses\n",
            "Saving subject02.happy to subject02.happy\n",
            "Saving subject02.leftlight to subject02.leftlight\n",
            "Saving subject02.noglasses to subject02.noglasses\n",
            "Saving subject02.normal to subject02.normal\n",
            "Saving subject02.rightlight to subject02.rightlight\n",
            "Saving subject02.sad to subject02.sad\n",
            "Saving subject02.sleepy to subject02.sleepy\n",
            "Saving subject02.surprised to subject02.surprised\n",
            "Saving subject02.wink to subject02.wink\n",
            "Saving subject03.centerlight to subject03.centerlight\n",
            "Saving subject03.glasses to subject03.glasses\n",
            "Saving subject03.happy to subject03.happy\n",
            "Saving subject03.leftlight to subject03.leftlight\n",
            "Saving subject03.noglasses to subject03.noglasses\n",
            "Saving subject03.normal to subject03.normal\n",
            "Saving subject03.rightlight to subject03.rightlight\n",
            "Saving subject03.sad to subject03.sad\n",
            "Saving subject03.sleepy to subject03.sleepy\n",
            "Saving subject03.surprised to subject03.surprised\n",
            "Saving subject03.wink to subject03.wink\n",
            "Saving subject04.centerlight to subject04.centerlight\n",
            "Saving subject04.glasses to subject04.glasses\n",
            "Saving subject04.happy to subject04.happy\n",
            "Saving subject04.leftlight to subject04.leftlight\n",
            "Saving subject04.noglasses to subject04.noglasses\n",
            "Saving subject04.normal to subject04.normal\n",
            "Saving subject04.rightlight to subject04.rightlight\n",
            "Saving subject04.sad to subject04.sad\n",
            "Saving subject04.sleepy to subject04.sleepy\n",
            "Saving subject04.surprised to subject04.surprised\n",
            "Saving subject04.wink to subject04.wink\n",
            "Saving subject05.centerlight to subject05.centerlight\n",
            "Saving subject05.glasses to subject05.glasses\n",
            "Saving subject05.happy to subject05.happy\n",
            "Saving subject05.leftlight to subject05.leftlight\n",
            "Saving subject05.noglasses to subject05.noglasses\n",
            "Saving subject05.normal to subject05.normal\n",
            "Saving subject05.rightlight to subject05.rightlight\n",
            "Saving subject05.sad to subject05.sad\n",
            "Saving subject05.sleepy to subject05.sleepy\n",
            "Saving subject05.surprised to subject05.surprised\n",
            "Saving subject05.wink to subject05.wink\n",
            "Saving subject06.centerlight to subject06.centerlight\n",
            "Saving subject06.glasses to subject06.glasses\n",
            "Saving subject06.happy to subject06.happy\n",
            "Saving subject06.leftlight to subject06.leftlight\n",
            "Saving subject06.noglasses to subject06.noglasses\n",
            "Saving subject06.normal to subject06.normal\n",
            "Saving subject06.rightlight to subject06.rightlight\n",
            "Saving subject06.sad to subject06.sad\n",
            "Saving subject06.sleepy to subject06.sleepy\n",
            "Saving subject06.surprised to subject06.surprised\n",
            "Saving subject06.wink to subject06.wink\n",
            "Saving subject07.centerlight to subject07.centerlight\n",
            "Saving subject07.glasses to subject07.glasses\n",
            "Saving subject07.happy to subject07.happy\n",
            "Saving subject07.leftlight to subject07.leftlight\n",
            "Saving subject07.noglasses to subject07.noglasses\n",
            "Saving subject07.normal to subject07.normal\n",
            "Saving subject07.rightlight to subject07.rightlight\n",
            "Saving subject07.sad to subject07.sad\n",
            "Saving subject07.sleepy to subject07.sleepy\n",
            "Saving subject07.surprised to subject07.surprised\n",
            "Saving subject07.wink to subject07.wink\n",
            "Saving subject08.centerlight to subject08.centerlight\n",
            "Saving subject08.glasses to subject08.glasses\n",
            "Saving subject08.happy to subject08.happy\n",
            "Saving subject08.leftlight to subject08.leftlight\n",
            "Saving subject08.noglasses to subject08.noglasses\n",
            "Saving subject08.normal to subject08.normal\n",
            "Saving subject08.rightlight to subject08.rightlight\n",
            "Saving subject08.sad to subject08.sad\n",
            "Saving subject08.sleepy to subject08.sleepy\n",
            "Saving subject08.surprised to subject08.surprised\n",
            "Saving subject08.wink to subject08.wink\n",
            "Saving subject09.centerlight to subject09.centerlight\n",
            "Saving subject09.glasses to subject09.glasses\n",
            "Saving subject09.happy to subject09.happy\n",
            "Saving subject09.leftlight to subject09.leftlight\n",
            "Saving subject09.noglasses to subject09.noglasses\n",
            "Saving subject09.normal to subject09.normal\n",
            "Saving subject09.rightlight to subject09.rightlight\n",
            "Saving subject09.sad to subject09.sad\n",
            "Saving subject09.sleepy to subject09.sleepy\n",
            "Saving subject09.surprised to subject09.surprised\n",
            "Saving subject09.wink to subject09.wink\n",
            "Saving subject10.centerlight to subject10.centerlight\n",
            "Saving subject10.glasses to subject10.glasses\n",
            "Saving subject10.happy to subject10.happy\n",
            "Saving subject10.leftlight to subject10.leftlight\n",
            "Saving subject10.noglasses to subject10.noglasses\n",
            "Saving subject10.normal to subject10.normal\n",
            "Saving subject10.rightlight to subject10.rightlight\n",
            "Saving subject10.sad to subject10.sad\n",
            "Saving subject10.sleepy to subject10.sleepy\n",
            "Saving subject10.surprised to subject10.surprised\n",
            "Saving subject10.wink to subject10.wink\n",
            "Saving subject11.centerlight to subject11.centerlight\n",
            "Saving subject11.glasses to subject11.glasses\n",
            "Saving subject11.happy to subject11.happy\n",
            "Saving subject11.leftlight to subject11.leftlight\n",
            "Saving subject11.noglasses to subject11.noglasses\n",
            "Saving subject11.normal to subject11.normal\n",
            "Saving subject11.rightlight to subject11.rightlight\n",
            "Saving subject11.sad to subject11.sad\n",
            "Saving subject11.sleepy to subject11.sleepy\n",
            "Saving subject11.surprised to subject11.surprised\n",
            "Saving subject11.wink to subject11.wink\n",
            "Saving subject12.centerlight to subject12.centerlight\n",
            "Saving subject12.glasses to subject12.glasses\n",
            "Saving subject12.happy to subject12.happy\n",
            "Saving subject12.leftlight to subject12.leftlight\n",
            "Saving subject12.noglasses to subject12.noglasses\n",
            "Saving subject12.normal to subject12.normal\n",
            "Saving subject12.rightlight to subject12.rightlight\n",
            "Saving subject12.sad to subject12.sad\n",
            "Saving subject12.sleepy to subject12.sleepy\n",
            "Saving subject12.surprised to subject12.surprised\n",
            "Saving subject12.wink to subject12.wink\n",
            "Saving subject13.centerlight to subject13.centerlight\n",
            "Saving subject13.glasses to subject13.glasses\n",
            "Saving subject13.happy to subject13.happy\n",
            "Saving subject13.leftlight to subject13.leftlight\n",
            "Saving subject13.noglasses to subject13.noglasses\n",
            "Saving subject13.normal to subject13.normal\n",
            "Saving subject13.rightlight to subject13.rightlight\n",
            "Saving subject13.sad to subject13.sad\n",
            "Saving subject13.sleepy to subject13.sleepy\n",
            "Saving subject13.surprised to subject13.surprised\n",
            "Saving subject13.wink to subject13.wink\n",
            "Saving subject14.centerlight to subject14.centerlight\n",
            "Saving subject14.glasses to subject14.glasses\n",
            "Saving subject14.happy to subject14.happy\n",
            "Saving subject14.leftlight to subject14.leftlight\n",
            "Saving subject14.noglasses to subject14.noglasses\n",
            "Saving subject14.normal to subject14.normal\n",
            "Saving subject14.rightlight to subject14.rightlight\n",
            "Saving subject14.sad to subject14.sad\n",
            "Saving subject14.sleepy to subject14.sleepy\n",
            "Saving subject14.surprised to subject14.surprised\n",
            "Saving subject14.wink to subject14.wink\n",
            "Saving subject15.centerlight to subject15.centerlight\n",
            "Saving subject15.glasses to subject15.glasses\n",
            "Saving subject15.happy to subject15.happy\n",
            "Saving subject15.leftlight to subject15.leftlight\n",
            "Saving subject15.noglasses to subject15.noglasses\n",
            "Saving subject15.normal to subject15.normal\n",
            "Saving subject15.rightlight to subject15.rightlight\n",
            "Saving subject15.sad to subject15.sad\n",
            "Saving subject15.sleepy to subject15.sleepy\n",
            "Saving subject15.surprised to subject15.surprised\n",
            "Saving subject15.wink to subject15.wink\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9p9DLKo_fIQ"
      },
      "source": [
        "Creating separate groups of images to be used as training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq5Z6N6__opl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd9a45b2-2969-43f3-c054-55b1b0a01777"
      },
      "source": [
        "!mkdir data\n",
        "!mv subject* data/.\n",
        "!rm -r training_data/\n",
        "!mkdir training_data\n",
        "!touch training_data/info.txt\n",
        "!rm -r test_data/\n",
        "!mkdir test_data\n",
        "!touch test_data/test.txt"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "mv: cannot stat 'subject*': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L22ELm6e_4HO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f29069-951f-41be-f3c8-8564488f910a"
      },
      "source": [
        "import os \n",
        "import shutil\n",
        "import numpy\n",
        "import cv2\n",
        "from random import randint\n",
        "\n",
        "all_img = open(\"training_data/info.txt\",'w')\n",
        "all_sub = []\n",
        "a = 1\n",
        "test_img = open(\"test_data/test.txt\",'w')\n",
        "test_sub = []\n",
        "t = 1\n",
        "\n",
        "# The image of each of the subjects are named as subjecti.emotions.png\n",
        "emotions = ['centerlight','glasses','happy','leftlight','noglasses','normal','rightlight','sad','sleepy','surprised','wink']\n",
        "for i in [\"subject01.\",\"subject02.\",\"subject03.\",\"subject04.\",\"subject05.\",\"subject06.\",\"subject07.\",\"subject08.\",\"subject09.\",\"subject10.\",\"subject11.\",\"subject12.\",\"subject13.\",\"subject14.\",\"subject15.\"]:\n",
        "  test = [randint(0,10),randint(0,10)]\n",
        "  \n",
        "  #Randomly selecting the images for the test set\n",
        "  if test[0]==test[1]: \n",
        "    if test[0]==10: test[1] = 9\n",
        "    else: test[1] = test[0]+1\n",
        "  for j in range(11):\n",
        "    if j in test:\n",
        "      cap = cv2.VideoCapture(os.path.join('data',i+emotions[j]))\n",
        "      ret, frame = cap.read()\n",
        "      if (ret):\n",
        "        #The algorithm works on gray scale images \n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        cv2.imwrite('test_data/'+str(t)+'.jpg',gray)\n",
        "        test_sub.append(i[7:9]+\"\\n\")\n",
        "        t = t+1\n",
        "    else:\n",
        "      cap = cv2.VideoCapture(os.path.join('data',i+emotions[j]))\n",
        "      ret, frame = cap.read()\n",
        "      if (ret):\n",
        "        #The algorithm works on gray scale images \n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        cv2.imwrite('training_data/'+str(a)+'.jpg',gray)\n",
        "        all_sub.append(i[7:9]+\"\\n\")\n",
        "        a = a+1\n",
        "      \n",
        "all_img.writelines(all_sub)\n",
        "all_img.close()\n",
        "test_img.writelines(test_sub)\n",
        "test_img.close()\n",
        "print(str(a+t-2)+\" images in total, are loaded\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "165 images in total, are loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYib00iF_8s9"
      },
      "source": [
        "Eigenfaces are extracted from the training data set and used to identify the images further. Principal component analysis is used to reduce the data in computation without compromising the output quality to a large extent. \n",
        "\n",
        "The number of components to be retained is decided internally based on the thershold provided, which is percentage (i.e. out of 100) value of the amount of information to be retained, and is calculated as the sum over the retained components, of the fractional magnitude of eigenvalue.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2EGNmsPALHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caf4bc5d-e0dd-42cd-b793-6aa5eaa61592"
      },
      "source": [
        "import os \n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "#Getting the Viola Jones Classifier - pretrained classifier\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# The test can be performed over either of the training or test sets\n",
        "r_file = open('test_data/test.txt','r')\n",
        "# r_file = open('training_data/info.txt','r')\n",
        "result = r_file.readlines()\n",
        "r_file.close()\n",
        "\n",
        "l_file = open('training_data/info.txt','r')\n",
        "labels = l_file.readlines()\n",
        "l_file.close()\n",
        "\n",
        "#Reading the dataset of images\n",
        "def load_img(folder):\n",
        "  images = []\n",
        "  num = len(os.listdir(folder))\n",
        "  for i in range(num-1):\n",
        "    img = cv2.imread(folder+'/'+str(i+1)+'.jpg')\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    images.append(img)\n",
        "  return images\n",
        "\n",
        "#Loading the dataset\n",
        "images = load_img('training_data')\n",
        "print(\"Training Dataset Loaded ........\")\n",
        "\n",
        "#Detection of faces using Voila Jones Cascade Classifier\n",
        "def detect_face(img):\n",
        "  # cv2_imshow(img)\n",
        "  roi = []\n",
        "  faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
        "  if(np.size(faces) > 0):\n",
        "    #If face detected, pass only the region of interest\n",
        "    for (x,y,w,h) in faces:\n",
        "      img = img[y:y+h, x:x+w]\n",
        "      roi = [x,y,w,h]\n",
        "    return img,roi\n",
        "  else:\n",
        "    #If no face detected\n",
        "    return [],roi \n",
        "    \n",
        "faces = []\n",
        "rect = []\n",
        "size = []\n",
        "for img in images:\n",
        "  face,roi = detect_face(img)\n",
        "  if(np.size(face) > 0):\n",
        "    faces.append(face)\n",
        "    rect.append(roi)\n",
        "    size.append(np.size(face,1))\n",
        "frame_size = np.max(size)\n",
        "\n",
        "# The cropped face area sizes may differ, thus resizing them to the same size\n",
        "for i in range(len(faces)):\n",
        "  faces[i] = cv2.resize(faces[i],(frame_size,frame_size))  \n",
        "  faces[i] = faces[i].reshape(frame_size*frame_size,1)\n",
        "\n",
        "print(\"Faces detected .........\")\n",
        "\n",
        "# cv2_imshow(faces[10])\n",
        "# comment out the reshaping command, to check the cropped out image \n",
        "\n",
        "def my_face_recognizer(faces, labels):\n",
        "  # Stack of n^2 by 1 matrices, m in number \n",
        "  faces = np.array(faces)\n",
        "  n = np.size(faces,1)\n",
        "  m = np.size(faces,0)\n",
        "  \n",
        "  faces = faces[:,:,0].T\n",
        "  \n",
        "  # Finding the sample mean (Average face)\n",
        "  frame_size = int(np.sqrt(n))\n",
        "  avg_face = np.average(faces, axis=1).reshape((n,1))\n",
        "  # avg_face = avg_face.reshape((frame_size,frame_size))\n",
        "  # The average face formed can be seen here\n",
        "  # cv2_imshow(avg_face)\n",
        "\n",
        "  print(\"Training Started ...........\")\n",
        "  # Normalising the faces\n",
        "  norm_faces = faces-avg_face\n",
        "\n",
        "  # Finding the Covariance matrix\n",
        "  # Performing A.T * A so that the size of the matrix is limited to M^2\n",
        "  # and does not blow up to N^2\n",
        "  Cov = np.matmul(norm_faces.T, norm_faces)\n",
        "  # Using numpy function to compute the eigenvalues and eigenvectors of the reduced covariance matrix computed here\n",
        "  values, vectors = np.linalg.eig(Cov)\n",
        "  vectors = np.append(vectors, np.reshape(values,(np.size(vectors,0),1)), axis=1)\n",
        "  vectors = vectors[np.argsort(vectors[:, -1])[::-1]]\n",
        "  values = vectors[:,-1]\n",
        "  vectors = vectors[:,:-1]\n",
        "  \n",
        "  for i in range(len(values)-1):\n",
        "    if(values[i]<values[i+1]):\n",
        "      print(\"Error!!!!!!!!!!!\")\n",
        "\n",
        "  # Computing the eigenvectors of the actual covariance matrix\n",
        "  vectors = np.matmul(norm_faces,vectors)\n",
        "  \n",
        "  #Deciding k\n",
        "  tot_eig = np.sum(values)\n",
        "  weightage = [(i/tot_eig)*100 for i in values]\n",
        "  coverage = np.cumsum(weightage)\n",
        "  threshold = 97\n",
        "  # %coverage of the variance, decides the number of Principal components to be used for the Dimensionality reduction\n",
        "  k = 1\n",
        "  while(coverage[k-1]<threshold):\n",
        "    k = k+1\n",
        "  print(\"K is decided to be: \"+str(k))\n",
        "\n",
        "  values = values[len(values)-k:]\n",
        "  vectors = vectors[:,np.size(vectors,1)-k:]\n",
        "\n",
        "  weights = np.matmul(vectors.T,norm_faces)\n",
        "  # weights = np.linalg.lstsq(vectors, norm_faces, rcond=-1)[0]\n",
        "  #Instead of solving the matrix equation, a simple matrix multiplication would also work, as eigenvectors involved are orthogonal to each other\n",
        "\n",
        "  # trained = np.matmul(vectors, weights)\n",
        "  trained_recognizer_model = {}\n",
        "  trained_recognizer_model[\"Average face\"] = avg_face\n",
        "  trained_recognizer_model[\"Weights\"] = weights\n",
        "  trained_recognizer_model[\"Eigenvector\"] = vectors\n",
        "  trained_recognizer_model[\"Eigenvalue\"] = values\n",
        "  return trained_recognizer_model\n",
        "\n",
        "trained_recognizer_model = my_face_recognizer(faces, labels)\n",
        "print(\"Training completed ...........\") \n",
        "\n",
        "def recognize(face,trained_recognizer_model):\n",
        "  # cv2_imshow(face)\n",
        "  face = face.reshape(frame_size*frame_size,1)\n",
        "  norm_face = face - trained_recognizer_model[\"Average face\"]\n",
        "  test_weights = np.matmul(trained_recognizer_model[\"Eigenvector\"].T,norm_face)\n",
        "  # test_weights = np.linalg.lstsq(trained_recognizer_model[\"Eigenvector\"], norm_face, rcond=-1)[0]\n",
        "  dis = np.linalg.norm(trained_recognizer_model[\"Weights\"]-test_weights, axis=0)\n",
        "  match = np.where(dis == np.min(dis))\n",
        "  return match\n",
        "  \n",
        "def test(test_data):\n",
        "  test_images = load_img(test_data)\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  for i in range(len(test_images)):\n",
        "    img = test_images[i]\n",
        "    face,roi = detect_face(img)\n",
        "    if (np.size(face) > 0):\n",
        "      total = total+1\n",
        "      face = cv2.resize(face,(frame_size,frame_size))\n",
        "      name = recognize(face,trained_recognizer_model)\n",
        "      # cv2_imshow(face)\n",
        "      # The actual result, and the predicted one, are printed by the following command\n",
        "      print(result[i] + \" : \" + labels[name[0][0]])\n",
        "      if (result[i] == labels[name[0][0]]):\n",
        "         correct = correct+1\n",
        "  else:\n",
        "      print(\"No face in the image\")\n",
        "  return correct/total\n",
        "\n",
        "print(\"Testing started ..........\")\n",
        "accuracy = test('test_data')\n",
        "# accuracy = test('training_data')\n",
        "print(\"Testing completed............\")\n",
        "print(\"Accuracy is: \"+str(accuracy*100)+\" %\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Loaded ........\n",
            "Faces detected .........\n",
            "Training Started ...........\n",
            "K is decided to be: 73\n",
            "Training completed ...........\n",
            "Testing started ..........\n",
            "01\n",
            " : 01\n",
            "\n",
            "01\n",
            " : 01\n",
            "\n",
            "02\n",
            " : 02\n",
            "\n",
            "02\n",
            " : 02\n",
            "\n",
            "03\n",
            " : 03\n",
            "\n",
            "03\n",
            " : 08\n",
            "\n",
            "04\n",
            " : 04\n",
            "\n",
            "04\n",
            " : 03\n",
            "\n",
            "05\n",
            " : 05\n",
            "\n",
            "05\n",
            " : 05\n",
            "\n",
            "06\n",
            " : 02\n",
            "\n",
            "06\n",
            " : 06\n",
            "\n",
            "07\n",
            " : 07\n",
            "\n",
            "07\n",
            " : 03\n",
            "\n",
            "08\n",
            " : 08\n",
            "\n",
            "08\n",
            " : 10\n",
            "\n",
            "09\n",
            " : 09\n",
            "\n",
            "09\n",
            " : 09\n",
            "\n",
            "10\n",
            " : 03\n",
            "\n",
            "10\n",
            " : 10\n",
            "\n",
            "11\n",
            " : 11\n",
            "\n",
            "11\n",
            " : 11\n",
            "\n",
            "12\n",
            " : 12\n",
            "\n",
            "12\n",
            " : 12\n",
            "\n",
            "13\n",
            " : 13\n",
            "\n",
            "14\n",
            " : 14\n",
            "\n",
            "14\n",
            " : 04\n",
            "\n",
            "15\n",
            " : 15\n",
            "\n",
            "15\n",
            " : 15\n",
            "\n",
            "No face in the image\n",
            "Testing completed............\n",
            "Accuracy is: 75.86206896551724 %\n"
          ]
        }
      ]
    }
  ]
}